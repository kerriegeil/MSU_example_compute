{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd2afb-ec78-4d2f-87c6-93a71ba2049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.distributed import Client,LocalCluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660741f-d719-41ae-a349-4abc72a64a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '/path/to/our/shared/datasets/folder/processed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d5157-c372-4283-a9ff-f4980381165d",
   "metadata": {},
   "source": [
    "# making linear regression go fast\n",
    "\n",
    "method: using dask distributed scheduler, xr.polyfit, and xr.map_blocks\n",
    "\n",
    "if you don't use distributed (client,cluster) and instead use default dask, this same thing takes 16 minutes\n",
    "\n",
    "only remaining issue: I cannot for my life suppress xarray polyfit rank warnings\n",
    "\n",
    "<br><br>\n",
    "general rules for fast linear reg comp that I've found in my testing\n",
    "1) always use dask distributed client,cluster\n",
    "2) use built in functions that do the math for you as much as possible\n",
    "3) compute linear reg in a way that can be vectorized, don't ever compute a single cell at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed54bd-1002-43c0-99d4-37d7e14fe2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers=20 # there are 2 threads per core so max we can pick for LocalCluster is 2x40=80\n",
    "cluster=LocalCluster(n_workers=nworkers,threads_per_worker=1) # a cluster where each thread is a separate process or \"worker\"\n",
    "client=Client(cluster)  # connect to your compute cluster\n",
    "client.wait_for_workers(n_workers=nworkers,timeout=10) # wait up to 10s for the cluster to be fully ready, error if not ready in 10s\n",
    "client # print info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecdd93-cec2-494b-b631-3affd6bea473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call with map_blocks\n",
    "# operates on xarray chunks and returns xarray chunks\n",
    "def polyfit_parallel(data_chunk,skipna):\n",
    "    data_chunk.coords['datetime']=data_chunk.time\n",
    "    data_chunk.coords['time']=data_chunk.datetime.dt.year\n",
    "    result_chunk = data_chunk.polyfit('time',1,skipna=skipna)\n",
    "    return result_chunk.polyfit_coefficients.sel(degree=1).drop_vars('degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21830180-e795-41e9-9f16-68ef8120546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# lazy load wealth to chunked array (not in memory)\n",
    "print('lazy load data')\n",
    "# this chunking results in wealth with 80 chunks, each about 75MB in size, not in memory yet\n",
    "wealth = xr.open_dataset(path_to_data+'wealth.nc',chunks={'time':-1,'y':1000,'x':1000}).wealth_pc\n",
    "\n",
    "# set up a chunked array template that has the exact dims and coords as the output of function polyfit_parallel will have\n",
    "print('create output template array')\n",
    "template=wealth.isel(time=0).drop_vars(['time','spatial_ref'])\n",
    "template.attrs={'standard_name':'wealth_pc trend'} # put whatever attributes you want\n",
    "\n",
    "# do the parallel compute\n",
    "print('execute')\n",
    "wealth_trend=wealth.map_blocks(polyfit_parallel,template=template,kwargs={'skipna':'True'}).compute()\n",
    "wealth_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df0741-44cf-4a6f-8010-ff2b8f003952",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96e818-f981-4058-a26f-55d5b6ccef13",
   "metadata": {},
   "source": [
    "I would think that this will also work with apply_ufunc and may be even faster since that deals with numpy arrays instead of labeled xr arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b306251-7ecf-4073-92cb-fe47936911d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_trend.min().item(),wealth_trend.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af8dee-0178-4a18-aa75-6470f022d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_trend.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89c6fb-6d2f-4fa4-8d0f-67299d10aff7",
   "metadata": {},
   "source": [
    "Is this the expected result? Most trend values are very close to zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sera_dataprep2",
   "language": "python",
   "name": "sera_dataprep2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
