{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863a0b34-069a-4cf0-9608-6f525ee6ac0d",
   "metadata": {},
   "source": [
    "# Python Dask Demonstration on HPC Orion\n",
    "\n",
    "Created: 7 Feb 2024\n",
    "\n",
    "By: Kerrie Geil, Associate Research Professor, Geosystems Research Institute, Mississippi State University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af63db-8c16-464e-8659-fc92254db1c7",
   "metadata": {},
   "source": [
    "**There are a few different ways to implement parallelization with python Dask on an HPC.** Dask includes a [few different schedulers](https://docs.dask.org/en/stable/scheduling.html) (schedulers send calculations to your cpu cores or threads). The code you write to implement dask with one vs the other scheduler is slightly different. Dask's single-machine scheduler is the default but is used mostly on pc's, it doesn't scale to an HPC environment. I prefer this scheduler on my laptop and workstation. For an HPC environment, Dask offers their distributed scheduler. And, the distributed scheduler has an option that will also run on a pc so that your code may be more transferable.\n",
    "\n",
    "Using the **single-machine scheduler** on a pc is generally more \"seamless\" i.e. less code to write, more things working auto-magically behind the scenes. The limitation to the single-machine scheduler is that you can only parallelize across cpu cores on a single machine. The single machine scheduler isn't meant to work great in an HPC environment.\n",
    "\n",
    "With the **distributed scheduler** in an HPC environment, you have a couple of options for building a cluster. When you set up a cluster you are defining how much computing power (cores, memory, time) you want to access. Dask's **SLURMCluster** function allows you to access multiple compute nodes simultaneously. Theoretically, you could access 10 entire nodes (400 cpu cores) per compute job. The biggest limitation to using SLURMCluster on HPC Orion is node availability and the overhead time it takes to start the cluster which is usually about 45-60 seconds. For SLURMCluster to work seamlessly, the cpu cores you request essentially need to be available immediately. This can't always be gauranteed on HPC Orion because there are a ton of other users on the system, so performance of SLURMCluster on Orion can be inconsistent or unpredictable. The other option for building a cluster is to use Dask's **LocalCluster** function. LocalCluster is kind of the HPC equivalent to using Dask's single-machine scheduler. LocalCluster allows you to access the cores and memory on a single node. On the orion partition this limits you to 40 cores and 192GB RAM. On the bigmem partition you'd be limited to 40 cores and 384GB RAM. Reminder, that info about partitions and memory, etc on Orion can be found in the [User Guide](https://intranet.hpc.msstate.edu/helpdesk/resource-docs/orion_guide.php) (username/password required). I find that even if you don't count the time it take SLURMCluster to start up, computing with LocalCluster is still a bit faster when running on 1 node's worth of cores. Another benefit to using the LocalCluster function is that it makes your code more transferable. Your code should also be able run in different HPC environments or on your own pc, it will just take longer because there are less cores.   \n",
    "\n",
    "**My personal approach to parallelization** is to first attempt using the single-machine scheduler on my pc. If I need more compute power (because a code is taking too long to run or I need more memory) then I have two choices. I could use the Dask distributed scheduler with LocalCluster or SLURMCluster from a Jupyter notebook or a .py file. Or I could not use Dask or Jupyter notebooks at all, instead opting to run .py files in batch mode. This would require programming more of the parallelization manually in shell scripts that call the .py files. This is the \"traditional\" way of using an HPC. I try to always opt for Jupyter notebooks and LocalCluster if possible. If I still need more computational power I'll try SLURMCluster. SLURMCluster is good if my computation only needs a few nodes and doesn't need to be run frequently since SLURMCluster can be inconsistent due to system usage. If you don't want to babysit your code or you have code that is more operational (runs every day) and needs to succeed every time, then I'd abandon Jupyter notebooks in favor of batched .py scripts. \n",
    "\n",
    "**Now, we'll cover a couple different Dask parallelization techniques using Dask's distributed schedulers. These examples work best with gridded data in netcdf, zarr, or npy stacks.** Dask can also handle dataframes (tabular/txt data), but that won't be covered here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e9414-1ec6-4b38-aad0-458bb6d7abeb",
   "metadata": {},
   "source": [
    "## Set up\n",
    "\n",
    "Generally it's good practice to import all your packages up front here.\n",
    "\n",
    "However, we won't do that here in order to make it more clear which packages we are using at each stage of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4ab02-fe1e-4e10-9504-661422482111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I would normally import here\n",
    "# import glob\n",
    "# from dask.distributed import Client,LocalCluster\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# import xarray as xr\n",
    "# import dask.array as da\n",
    "# import dask\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93285d4-b9a9-47c6-8139-b95c917d1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory and file paths\n",
    "\n",
    "datadir='/path/to/dir/where/Tmax/netcdf/lives/'\n",
    "outdir='/path/to/dir/in/your/personal/workspace/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd910d-2f22-4ca7-8899-a2434d3d131e",
   "metadata": {},
   "source": [
    "# Dask distributed scheduler using LocalCluster\n",
    "\n",
    "Computing on a single node. You will have access to the node you launched jupyer with. When you are going to use LocalCluster, my recommendation for launching Jupyter is to use number of nodes 1, number of tasks 40, additional slurm parameters --exclusive on the launch page.\n",
    "\n",
    "We launched Jupyter on 1 node with 40 cores. On Orion each core has 2 threads. So the maximum threads we can have using Dask LocalCluster is 80. We won't actually need all those cores and threads for this notebook though. We'll set up our LocalCluster with 1/4 of the compute power available on our node: 20 Dask workers where each worker is 1 thread. Dask calls these parameters n_workers and threads_per_worker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17837a-d73e-4256-aeab-43842c968543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('importing...')\n",
    "from dask.distributed import Client,LocalCluster\n",
    "\n",
    "print('starting client...')\n",
    "nworkers=20\n",
    "cluster=LocalCluster(n_workers=nworkers,threads_per_worker=1)#, memory_limit=\"4.5GiB\") # a cluster where each thread is a separate process or \"worker\"\n",
    "client=Client(cluster)  # connect to your compute cluster\n",
    "client.wait_for_workers(n_workers=nworkers,timeout=10) # wait up to 10s for the cluster to be fully ready, error if not ready in 10s\n",
    "client # print info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff284e2-9693-4fdc-8f2f-4bd3b07d8754",
   "metadata": {},
   "source": [
    "### dask arrays\n",
    "\n",
    "Dask arrays are great if your data is in netcdf or zarr format or npy stacks (usually gridded data).\n",
    "\n",
    "Data in a Dask array is not held in memory. Also, Dask arrays are chunked (like divided into smaller subsets of the array). This allows for easy parallelization by spreading the chunks across many cpu cores when it comes time to execute a calculation.\n",
    "\n",
    "Problems may arise for complicated calculations e.g. when a calculation requires calling multiple custom functions or subroutines and there are many variables involved. The problem usually arises because these type of calculations require holding too much data in memory. Behind the scenes, Dask attempts to estimate memory needs for every task, but when calculations get complicated sometimes Dask can't estimate memory needs well enough. In this situation, it's better to use dask delayed (demonstrated later).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982557d4-4569-41f7-968b-e3c6bc10e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# glob is an easy way to get a list of multiple files\n",
    "# we'll only use 1 of the files so this is just for demonstration purposes\n",
    "f=glob.glob(datadir+'*.nc')\n",
    "len(f),f[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a2a05-62f8-4d5e-a3cd-c425217d3e87",
   "metadata": {},
   "source": [
    "#### let's talk chunking\n",
    "\n",
    "Chunking means dividing up your array of data into separate parts in one or more dimensions.\n",
    "\n",
    "For xarray, when you provide the chunk parameter your xarray object is backed by a dask array which is not held in memory. This makes computing with data bigger than memory easy. In the examples here our data is not bigger than memory, but the code scales.\n",
    "\n",
    "My approach, when it's possible, is to choose a total number of chunks that is equal to the number of cores/threads that I have. This is when data is not small but can still fit into memory. If data is bigger than memory, I choose a total number of chunks that is a multiple of the cores/threads that I have. What dask does is cycle through the chunks, sending 1 or more chunks at a time to each core/thread.\n",
    "\n",
    "It's not always best to make nchunks = ncores or nthreads though. The size (bytes) of a chunk also makes a difference. You don't want to make the chunks too small because the cycling through chunks adds overhead. For small data it will be faster to compute without chunking at all. You also don't want to make chunks too big or you could run out of memory as the cores compute. So how do you choose a good size chunk? Really it's trial and error. On HPC Orion, a compute node on the orion paritition has about 190GB of RAM shared across the 40 cores on each node. If you have 40 or more chunks and are operating on 1 node with 40 cores, you want to make sure your calculation on a single chunk won't need more RAM than is available on a single core (which is 190/40=4.75GB if you're using all cores). It is often not easy to estimate how much RAM will be used by a calculation, especially if the calculation is complex. Some good best practices for RAM management in your code are to avoid double precision data types when possible and to delete old variables that are hanging in memory after certain steps of a calculation that won't be used again. Personally, I usually try a couple different chunk sizes between 100MB and 1GB to see what ends up being fastest.\n",
    "\n",
    "Here, I know that my data dimensions are 'time':365, 'lat':1800, 'lon':4320. Xarray's chunks parameter sets the chunk dimensions. So if I want to chunk over the longitude dimension and I want 18 chunks the xarray parameter is chunks={'time':-1,'lat':-1,'lon':240} where 240 comes from 4320/18. You could also set chunks as chunks={'time':365,'lat':1800,'lon':240} or chunks={'lon':240} without explicit mention of the other dimensions. All of these are interpreted identically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30893842-e144-479e-9bb9-af6298d3cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open chunked file\n",
    "# ds is an xarray data structure filled with dask arrays\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "chunks={'time':-1,'lat':-1,'lon':240}\n",
    "\n",
    "ds=xr.open_mfdataset(f[0],chunks=chunks,lock=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca88b7-cc7e-4068-9d8c-5f5fc969aaf1",
   "metadata": {},
   "source": [
    "### using built-in functions on dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d522e2-4460-40db-9d1d-c250f92e53ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# send information about our data chunks and computation tasks\n",
    "# here there were 2 tasks (layers) in the graph which are load and chunk\n",
    "# persist starts move the data chunks to the workers in the background\n",
    "# var=ds.Tmax.persist()\n",
    "var=ds['Tmax-2m'].persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a5623-7900-4c4d-b81f-263ef3f8a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# most calculations will be lazy if you don't include .compute()\n",
    "# lazy means the compute task is recorded on the dask graph but not executed\n",
    "# .compute() is what executes the calculation\n",
    "var_mean=var.mean('time')\n",
    "var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d50e6-7d24-42e0-bed3-e533b025a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute the calculation\n",
    "var_mean=var_mean.compute()\n",
    "var_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db87bb3-2ed9-48f7-a8f7-452da6e0d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above steps can also be combined into a single line\n",
    "\n",
    "var_mean=var.mean('time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c67f2e-d8a1-4612-b583-d52b49118dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e91fc7-8d4f-4192-b8d1-f4396da00485",
   "metadata": {},
   "source": [
    "what about calculations that are more than one line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a2bb0-259d-4892-a7fb-d46f7a7456b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# you can string multiple calculations together and call compute only on the last variable to execute everything\n",
    "var_monthly=var\\\n",
    "            .groupby('time.month')\\\n",
    "            .mean('time') # monthly means\n",
    "month_minval=var_monthly\\\n",
    "            .min('month') # minimum of monthly means\n",
    "month_minval.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866b394-4134-42c2-a81f-c13db87a04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_minval.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a33e3-41be-4161-b8bc-fff6b0d53b9b",
   "metadata": {},
   "source": [
    "#### custom compute functions on dask arrays\n",
    "\n",
    "when we have written our own functions we can apply them to dask arrays with .map_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198dbef-bd1c-4ac8-9451-1860111a5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function could contain anything, but we'll keep it simple here\n",
    "def my_function(x):\n",
    "    newval=x.mean('time')\n",
    "    return newval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da5656-725e-4bad-8f88-3543e76d6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# xarray map_blocks\n",
    "# var is technically an xarray object even though it's backed with dask arrays\n",
    "# so map_blocks here is from the xarray library, see more at https://docs.xarray.dev/en/stable/generated/xarray.map_blocks.html\n",
    "varmean=var.map_blocks(my_function).compute()\n",
    "varmean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff7d58-c2bb-49a7-b61b-cf724dd75ce4",
   "metadata": {},
   "source": [
    "#### custom compute functions on overlapping chunks\n",
    "\n",
    "where we want to apply convolutions we can use dask.array.map_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547bb5d4-5acf-4220-b649-10cbb0a0f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# imagine this function is applying a convolution filter or something more complicated\n",
    "# we keep it simple here\n",
    "def my_mult_function(x):\n",
    "    newval=x*3.\n",
    "    return newval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286270e-fa04-41c5-9820-4f509d395b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this works if my_function doesn't change the data shape ###\n",
    "import dask.array as da\n",
    "\n",
    "# the dask function .map_overlap takes dask/numpy arrays as input, not xarray data structures\n",
    "# our xarray data structure is already backed by dask/numpy arrays (because we accessed the nc file with xr.open_mfdataset with chunks) so\n",
    "# all we have to do to access the data in dask/numpy format is to use the .data method on our xarray object\n",
    "\n",
    "# we want our calculation to happen on overlapping chunks where \n",
    "#  - 3 pixels of each chunk overlaps (depth=3),\n",
    "#  - no calculation happens at the chunk boundaries (boundary='none'),\n",
    "#  - and the overlap pixels are trimmed off of each chunk after the calculation (trim=True)\n",
    "\n",
    "# remember this just adds to the dask graph, does not actually compute\n",
    "varmult=da.map_overlap(my_mult_function, var.data, dtype=np.float32, depth=3, boundary='none', trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0443852-41cf-4712-a307-529bbc8d170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# do the computation\n",
    "# compute brings the result into memory in the form of a numpy array\n",
    "# we are only multiplying, so the computation should return\n",
    "# a numpy array with 3 dimensions\n",
    "varmult=varmult.compute()\n",
    "varmult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff32dd6-9176-4a8e-bf4f-6aec02748d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the first time of the varmult numpy array\n",
    "\n",
    "plt.imshow(varmult[0,:,:],interpolation='none',cmap='coolwarm')\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d7df5-85ae-460d-a397-4db0a484fa32",
   "metadata": {},
   "source": [
    "#### try with a function that changes the data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4782de-d2f2-438f-919f-6fc170727afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function accepts data of shape (time, lat, lon)\n",
    "# applies the .mean function\n",
    "# and returns data of shape (lat,lon)\n",
    "\n",
    "# imagine though that we could be applying a convolution filter here\n",
    "# then doing more calculations that end up reducing the dimensions of our input data\n",
    "def my_mean_function(x):\n",
    "    newval=np.mean(x,axis=0)\n",
    "    return newval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cc63e-a41c-43eb-bbe0-65ae5e6e1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# dask.map_overlap is using dask.map_blocks under the hood meaning that\n",
    "# .map_overlap accepts parameters for .map_blocks as **kwargs\n",
    "# we saw above with varmult that .map_overlap works without any kwargs if my_function doesn't change the shape of the data\n",
    "# since .mean does change the shape of the data from (time, lat, lon) to (lat,lon) we have to use a kwarg \"drop_axis\"\n",
    "# see the dask pages for .map_overlaps and .map_blocks for more info\n",
    "# all the possible kwargs you can use will be listed on the .map_blocks page\n",
    "\n",
    "kwargs={'drop_axis':0} # kwargs for .map_blocks, put them in a python dictionary {'key':value} here\n",
    "\n",
    "# same thing as before except call our mean function and include the kwargs\n",
    "varmean=da.map_overlap(my_mean_function, var.data, dtype=np.float32, depth=3, boundary='none', trim=True, **kwargs)\n",
    "varmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f62a9-57d9-44ef-856b-ee6055e4cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varmean=varmean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81bf418-9ed8-4e1e-9ec7-635669fea311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(varmean,interpolation='none',cmap='coolwarm')\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e00fcd-b740-47cf-a303-ba5aa476f557",
   "metadata": {},
   "source": [
    "### dask delayed\n",
    "\n",
    "Dask has a few other features for executing computations. Sometimes, if we string too many calculations together on a dask array, dask can get confused. In these cases it's better to use Dask delayed. Dask delayed also works well for custom written functions.  \n",
    "\n",
    "Dask delayed operates on numpy arrays not xarray. Here we'll just convert from xarray backed with dask arrays to dask array backed with numpy arrays (still not in memory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6728e-f364-4a49-a6d8-ba2efeef5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_np=var.data # convert to dask/numpy\n",
    "var_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e5c62-7ee4-40df-88f8-e4cc0b3afbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to numpy means all our labels get deleted\n",
    "# so we can't use .mean('time') from the xarray library anymore https://docs.xarray.dev/en/stable/generated/xarray.DataArray.mean.html\n",
    "# we have to use .mean(axis=0) from the numpy library https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "\n",
    "# this function could contain anything, but we'll keep it simple here\n",
    "def my_np_function(x):\n",
    "    newval=np.mean(x,axis=0)\n",
    "    return newval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008609c7-d5f1-4653-b01c-91db402fa0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for large arrays we delay them first\n",
    "# this reduces moving big data across the workers by just moving chunks where they are needed instead of moving the whole array\n",
    " \n",
    "var_delay=var_np.to_delayed().ravel() # make each chunk a delayed object\n",
    "var_delay  # the output shows all the chunks as dask delayed objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b381b844-9d65-4e64-b129-a48cc707a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "task_list=[dask.delayed(my_np_function)(var_chunk) for var_chunk in var_delay] # create a list of delayed compute tasks\n",
    "# what we've done above is called a list comprehension in python, see more at https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions\n",
    "task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac862653-882b-450f-a46d-cbadcb449144",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do the computation\n",
    "result_chunks=dask.compute(*task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac324f98-4955-431d-9ece-1aee84684dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results is a list of 18 arrays, 1 array for each chunk\n",
    "len(result_chunks),result_chunks[0],result_chunks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354d0b5-ebe3-4f92-839e-68a6be32b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reassemble into a single array we concatenate\n",
    "import numpy as np\n",
    "result=np.concatenate(result_chunks,axis=1) # put the chunks together along the longitude dimension axis 1\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513b428-44b4-4e90-a860-049f42d91bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to plot this numpy array\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(result,interpolation='none',cmap='coolwarm')\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea14884-79ee-4f67-ad66-4c0acaa5afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to convert back to xarray\n",
    "new_xr_var = xr.DataArray(result,dims=var_mean.dims,coords=var_mean.coords)\n",
    "new_xr_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd7882-bd9a-48a2-bff1-51bf5d8fd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xr_var.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beec068-1f5f-4510-9085-4a5184352a4f",
   "metadata": {},
   "source": [
    "There are plenty of ways to make xarray plots look nicer too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42fabd-f4c9-4090-83e9-cd0857bfb129",
   "metadata": {},
   "source": [
    "### write results to a new netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3dc5f-057d-4d01-b3aa-54d1a0c84344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert xarray data array to xarray dataset\n",
    "varname='tmax_mean'\n",
    "ds_out=new_xr_var.to_dataset(name=varname) # must give the variable in the dataset a name\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc19794-cc0b-434d-82ef-59377ad7d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign some more metadata: variable attributes and spatial reference copied from the .nc file we read in originally\n",
    "\n",
    "ds_out[varname].attrs=ds['Tmax-2m'].attrs\n",
    "ds_out=ds_out.assign_coords({'spatial_ref':ds.spatial_ref[0]})\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ac845-7233-44ef-8c9b-738d12bf72f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few more things to make writing the netcdf work\n",
    "\n",
    "time_encoding={'calendar':'standard','units':'days since 1900-01-01 00:00:00','_FillValue':None}\n",
    "lat_encoding={'_FillValue':None}\n",
    "lon_encoding={'_FillValue':None}\n",
    "var_encoding = {'zlib':True,'dtype':'float32'}    \n",
    "\n",
    "\n",
    "import getpass\n",
    "user=getpass.getuser()\n",
    "outfile=outdir+'dask_demo_output_'+user+'.nc'\n",
    "\n",
    "ds_out.to_netcdf(outfile,\n",
    "                encoding={'lat':lat_encoding,\n",
    "                      'lon':lon_encoding,\n",
    "                      'time':time_encoding,\n",
    "                      varname:var_encoding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420db52-dbb7-4b77-8dc7-c29606ea3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't ever want to connect to multiple clients at once, always shutdown your client before starting a new one\n",
    "# restarting the jupyter kernel (from the kernel menu) also works to shutdown a client, but will clear out your whole notebook too\n",
    "\n",
    "client.shutdown()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd825b-c4bc-48ee-bb25-01298f3dcfc2",
   "metadata": {},
   "source": [
    "# Dask distributed scheduler using SLURMCluster\n",
    "\n",
    "setting up the cluster is different\n",
    "\n",
    "where the code runs is different\n",
    "\n",
    "the rest of the code is the same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeeefa0-67f2-4fb0-b754-186e07fb19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this deletes all the variables in your notebook\n",
    "# I never really use this except in demo notebooks\n",
    "%reset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c130bd-7965-4512-a6c2-880e47ab01cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER NEEDS TO MODIFY QUEUE AND ACCOUNT PARAMETERS OF SLURMCluster BEFORE RUNNING\n",
    "\n",
    "# some of these things we've already imported\n",
    "# we don't need to import things again\n",
    "# but I'm doing it anyway as a reminder of what packages we're using\n",
    "from dask_jobqueue import SLURMCluster  # this import is new\n",
    "from dask.distributed import Client\n",
    "from time import time, sleep\n",
    "import os  # this is new too\n",
    "\n",
    "# make a folder in your home directory for all the logs that SLURM spits out\n",
    "logpath='~/dask-worker-space-can-be-deleted'\n",
    "if not os.path.exists(logpath):\n",
    "    os.makedirs(logpath)\n",
    "\n",
    "# this can be thought of as 1 worker\n",
    "cluster = SLURMCluster(\n",
    "    queue='xxxxx',\n",
    "    account=\"xxxxxx-xxxxx\",\n",
    "    processes=1,\n",
    "    cores=2,\n",
    "    memory='9GB',\n",
    "    walltime=\"00:20:00\",\n",
    "    log_directory=logpath)\n",
    "\n",
    "client=Client(cluster) # connect to cluster\n",
    "\n",
    "# I choose 18 workers here because that's how many chunks I have in my data \n",
    "# If I chose 80 workers the compute wouldn't go any faster because I only have 18 chunks to compute\n",
    "# This data isn't very big (~10GB) so making smaller chunks on more workers doesn't speed things up (I tried)\n",
    "nworkers=18  \n",
    "cluster.scale(nworkers) # increase the size of the cluster to 18 workers\n",
    "client.wait_for_workers(n_workers=nworkers,timeout=120) # wait up to 2 min for the cluster to be fully ready, error if not ready in 2min\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b1bf4-f006-4366-b5be-5b6850702f23",
   "metadata": {},
   "source": [
    "### using built-in functions on dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd19e75-69fa-457e-bd11-8f369f857a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# open chunked file\n",
    "# ds is an xarray data structure filled with dask arrays\n",
    "\n",
    "import glob\n",
    "import xarray as xr\n",
    "\n",
    "datadir='/path/to/dir/where/Tmax/netcdf/lives/'\n",
    "outdir='/path/to/dir/in/your/personal/workspace/'\n",
    "\n",
    "f=glob.glob(datadir+'*.nc')\n",
    "\n",
    "chunks={'time':-1,'lat':-1,'lon':240}\n",
    "\n",
    "ds=xr.open_mfdataset(f[0],chunks=chunks)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45883a-9f15-4980-b6ff-7ca7f945f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# send information about our data chunks and computation tasks\n",
    "# here there were 2 tasks (layers) in the graph which are load and chunk\n",
    "# persist starts move the data chunks to the workers in the background\n",
    "# most calculations will be lazy if you don't include .compute()\n",
    "# lazy means the compute task is recorded on the dask graph but not executed\n",
    "# .compute() is what executes the calculation\n",
    "var=ds['Tmax-2m'].persist()\n",
    "var_mean=var.mean('time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bf360-7321-4494-9926-518eea7732be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# you can string multiple calculations together and call compute only on the last variable to execute everything\n",
    "var_monthly=var.groupby('time.month').mean('time') # monthly means\n",
    "month_minval=var_monthly.min() # minimum of monthly means\n",
    "month_minval.compute()\n",
    "month_minval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2143f7-ee29-48d1-a62b-3efc14135056",
   "metadata": {},
   "source": [
    "### using custom functions on dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48dc7f5-368e-4947-9948-b4f21cda14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this function could contain anything, but we'll keep it simple here\n",
    "def my_function(x):\n",
    "    newval=x.mean('time')\n",
    "    return newval\n",
    "\n",
    "varmean=var.map_blocks(my_function).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa0692-966d-4a7b-8953-7e2b625e834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "varmean.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee27b8-4968-4c5d-bf05-5b4c77e91568",
   "metadata": {},
   "source": [
    "### dask delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa66bf-aace-4254-be9c-ef859497046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# converting to numpy means all our labels get deleted\n",
    "# so we can't use .mean('time') from the xarray library anymore https://docs.xarray.dev/en/stable/generated/xarray.DataArray.mean.html\n",
    "# we have to use .mean(axis=0) from the numpy library https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "\n",
    "import dask\n",
    "\n",
    "# this function could contain anything, but we'll keep it simple here\n",
    "def my_np_function(x):\n",
    "    newval=x.mean(axis=0) # 0 is the index of the time dimension\n",
    "    return newval\n",
    "\n",
    "var_np=var.data # convert to dask/numpy\n",
    "var_delay=var_np.to_delayed().ravel() # make each chunk a delayed object\n",
    "task_list=[dask.delayed(my_np_function)(var_chunk) for var_chunk in var_delay] # create a list of delayed compute tasks\n",
    "result_chunks=dask.compute(*task_list)\n",
    "result=np.concatenate(result_chunks,axis=1) # put the chunks together along the longitude dimension axis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc2822-b0e3-4c0a-95d3-d6b2f34cd3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(result,interpolation='none',cmap='coolwarm')\n",
    "plt.colorbar(shrink=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771b015-71c3-479f-94ec-5fbeb702ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprep",
   "language": "python",
   "name": "dataprep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
