{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc62cfb-c9be-4a41-84f1-78b4affa827a",
   "metadata": {},
   "source": [
    "# Statistics and Scientific Computing Examples\n",
    "\n",
    "Created: March 2024\n",
    "\n",
    "By: Kerrie Geil, Associate Research Professor, Geosystems Research Institute, Mississippi State University\n",
    "\n",
    "----\n",
    "\n",
    "You will need to have the following packages in your conda environment to run this notebook: \n",
    "\n",
    "numpy, xarray, pandas, scipy, matplotlib, cartopy, scikit-learn, netcdf4, ipykernel\n",
    "\n",
    "Most of these packages should already be in your dataprep environment but you will probably have to install cartopy and scikit-learn and possibly scipy.\n",
    "\n",
    "Hint: to check the list of packages installed in your conda environment, first activate the environment at the command line, then command: conda list\n",
    "\n",
    "**Important: open this notebook on a development node the first time you run it** because cartopy requires internet access to download coastlines for plotting.\n",
    "\n",
    "Topics Covered\n",
    "- Calculating anomalies\n",
    "- Pearson r correlation (multiple packages)\n",
    "- Time dimension rolling mean\n",
    "- Fancy indexing\n",
    "- Stacking and masking\n",
    "- Focal operation package suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9686356-52c5-4c67-8a32-825b4c233f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import numpy.testing as npt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import NaturalEarthFeature\n",
    "from sklearn.feature_selection import r_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03746945-b808-41c1-9ef5-65d7c635898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir='/path/to/our/shared/datasets/dir/'\n",
    "outputdir=basedir+'temporary/stats_demo_outputs/'\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "    \n",
    "year_start='1950'\n",
    "year_end='2023'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0be02-41dd-4817-96c2-1ee55dc3ec38",
   "metadata": {},
   "source": [
    "# Read Nino 3.4 Index from a text file\n",
    "\n",
    "1949-2023 (75 years, 12 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678d670-0960-4a63-8826-de9ecebc51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data file contains rows with the year and 12 monthly anomaly values for the Nino 3.4 area \n",
    "# skip the first column of years and read in the other 12 columns of data (python indexing is not inclusive at the end)\n",
    "# the base period for the anomalies is 1981-2010\n",
    "# data=np.loadtxt(basedir+'temporary/climate_indices/NOAA-PSL_nino3.4.txt', usecols=(np.arange(1,13)))\n",
    "nino_ind_raw=np.loadtxt(basedir+'temporary/climate_indices/NOAA-PSL_nino3.4_anomaly.txt', usecols=(np.arange(1,13)))\n",
    "\n",
    "# collapse data to a 1D timeseries\n",
    "nino_ind=nino_ind_raw.flatten()\n",
    "\n",
    "# print some info\n",
    "print(nino_ind_raw.shape) # see that the data is 2D\n",
    "print(nino_ind.shape) # now it's 1D\n",
    "print(nino_ind.min(),nino_ind.max()) # data value range\n",
    "nino_ind[0:24] # first two years of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140eb60b-002b-4e83-9d00-06b18a91ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array of datetimes to match the nino_ind data\n",
    "time=pd.date_range('1949-01','2023-12',freq='MS')\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d8676-66b6-4381-98c3-ada5ae79a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(15,2))\n",
    "plt.plot(time,nino_ind, linewidth=1)\n",
    "plt.axhline(y=0., color='grey', linestyle='--', linewidth=0.5)\n",
    "plt.title('Nino3.4 Index Anomaly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f2711e-3f7a-470d-82d2-855f87afffc4",
   "metadata": {},
   "source": [
    "# Read NCEP-NCAR-1 Reanalysis precipitation monthly mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a65dc2-6933-4591-bc52-76f96a286968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access all the data in the netcdf file as an xarray dataset\n",
    "# note: for tif files you can use xr.open_dataset with engine='rasterio' or rioxarray rio.open_rasterio\n",
    "ds=xr.open_dataset(basedir+'temporary/NCEP-NCAR-1/prate.sfc.mon.mean.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d3f84-06b7-4040-b89a-d2a926175bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the precip data out as an xarray data array and subset in time\n",
    "pr=ds.prate.sel(time=slice(year_start,year_end))\n",
    "\n",
    "# adjust precip data\n",
    "pr=pr*86400.               # kg/m2/s --> mm/day\n",
    "pr.attrs['units']='mm/day' # update metadata \n",
    "pr=xr.where(pr<0.,0,pr)    # anywhere pr is negative fill with zero\n",
    "pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727307e2-677e-4e09-a89f-05e5e8112109",
   "metadata": {},
   "source": [
    "# Calculate anomalies in the precip data\n",
    "\n",
    "using a base periodo 1981-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f41e9a-2348-4705-b617-392c8efcd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the climatological values (30 year mean for each month)\n",
    "clim= pr.sel(time=slice('1981','2010')).groupby('time.month').mean()\n",
    "clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ddd77-cd13-4adf-a157-c5d7bfdf6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomalies\n",
    "pr_anom=pr.groupby('time.month') - clim\n",
    "pr_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992ccd6-b93c-4561-a001-33a01b5357c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot anomalies for a single month\n",
    "\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree()) # projection comes from cartopy package\n",
    "ax.coastlines(linewidth=0.5)  # coastlines come from cartopy package      \n",
    "\n",
    "cbar_kwargs = {'label':'pr anomaly (mm/day)'} # modifications to the colorbar\n",
    "\n",
    "pr_anom.sel(time='1990-08-01').plot(ax=ax,cmap='bwr_r',cbar_kwargs=cbar_kwargs) # this is xarray plotting which is based on matplotlib package\n",
    "plt.title('August 1990')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64d644-805c-4e8b-aa7a-d62de646d76e",
   "metadata": {},
   "source": [
    "# Correlate precip anomalies with nino 3.4 index\n",
    "\n",
    "#### pearson correlation of nino1D timeseries (1950-2023) with every pixel timeseries (1950-2023) of pr_anom\n",
    "\n",
    "\n",
    "A number of packages have functions you can use to compute pearson correlation.\n",
    "\n",
    "- **scipy:** [scipy.stats.pearsonr](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html), inputs can be xarray or numpy arrays but this function only operates on 1D inputs so you have to loop through data points which could be slow. This function also returns the one- or two-sided pvalue and confidence interval. I use this when I looking to get a quick estimation of the p values.\n",
    "\n",
    "- **xarray:** [xr.corr](https://docs.xarray.dev/en/stable/generated/xarray.corr.html), you need both arrays to be xarray objects with a shared dimension and it outputs the Pearson correlation without any additional statistical information. It does operate on multiple dimensions so you can input two 3D arrays (time,lat,lon) and correlate over time to easily produce a point-to-point correlation map. I use this when I'm looking to do quick point-by-point correlations between multiple 3D data variables.\n",
    "\n",
    "- **numpy:** [np.corrcoef](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html), takes numpy arrays inputs that must be the same shape and it outputs the correlation matrix without any additional statistical information. Only operates on 1D and 2D arrays so if you have a array with (time,lat,lon) you need to collapse the space dimensions and then reshape back to 3D after the correlation. You also have to do extra work to pull the correlation values out of the matrix.\n",
    "\n",
    "- **scikit learn:** [sklearn.feature_selection.r_regression](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.r_regression.html#sklearn.feature_selection.r_regression), takes numpy or xarray arrays but only operates on either two 1D array inputs or one 1D and one 2D array inputs, so you need to collapse the space dimension and you can't correlate 3D variables easily. It returns the correlation coefficients without any additional statistical info.\n",
    "\n",
    "- **custom written funcion:** write your own function for both pval (and r if you want) if you are interested in modifying to account for autocorrelation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dcf10a-7653-4538-b020-e69c3dc5b48d",
   "metadata": {},
   "source": [
    "#### scipy.stats.pearsonr example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc29b6-2520-4fc9-8fce-1bdb6af54c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=nino_ind[12:]  # subset time to start at 1950, this is a numpy array\n",
    "X=pr_anom.stack(space=('lat','lon'))  # reshape to 2D xarray ('time','space')\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0704feb0-3bee-4d53-aca7-b26f977d1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[scipy.stats.pearsonr(X[:,ipt],y) for ipt in range(X.shape[1])]  # loop thru space points and return results in a list\n",
    "len(results), results[0], results[0].statistic, results[0].confidence_interval(confidence_level=0.95)  # examples of how to access the info returned      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0b5e9-8e45-4cc8-9bd1-489637b56c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the results back into arrays for plotting\n",
    "\n",
    "r_list=[results[ipt].statistic for ipt in range(len(results))] # pull r into its own list\n",
    "r=xr.DataArray(r_list,dims=['space'],coords={'space':X.space}).unstack() # create xarray object with lat and lon labels\n",
    "\n",
    "# do the same for p\n",
    "p_list=[results[ipt].pvalue for ipt in range(len(results))] # pull p into its own list\n",
    "p=xr.DataArray(p_list,dims=['space'],coords={'space':X.space}).unstack() # create xarray object with lat and lon labels\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687c07f-faee-4d61-9591-17fa255d6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation masked with pvalue\n",
    "\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree()) # projection comes from cartopy package\n",
    "ax.coastlines(linewidth=0.5)  # coastlines come from cartopy package      \n",
    "\n",
    "cbar_kwargs = {'label':'pearson r'} # modifications to the colorbar\n",
    "\n",
    "siglvl=0.05\n",
    "r.where(p<=siglvl).plot(ax=ax,cmap='RdBu',cbar_kwargs=cbar_kwargs) # this is xarray .where and .plot\n",
    "plt.title('Correlation between monthly precip and nino3.4 anomalies, p<='+str(siglvl))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2231db-dfc6-450b-a8be-4ba1d022f810",
   "metadata": {},
   "source": [
    "#### xr.corr example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8617f0-43e5-49e7-839b-000a656b4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with xarray\n",
    "\n",
    "y_xr=xr.DataArray(y,dims='time',coords={'time':time[12:]}) # convert numpy array to xarray\n",
    "r_xr=xr.corr(X,y_xr,dim='time').unstack()\n",
    "\n",
    "# plot correlation\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree()) # projection comes from cartopy package\n",
    "ax.coastlines(linewidth=0.5)  # coastlines come from cartopy package      \n",
    "\n",
    "cbar_kwargs = {'label':'pearson r'} # modifications to the colorbar\n",
    "\n",
    "r_xr.plot(ax=ax,cmap='RdBu',cbar_kwargs=cbar_kwargs) # this is xarray .where and .plot and colorbar properties have to be passed here with kwargs\n",
    "plt.title('Correlation between monthly precip and nino3.4 anomalies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ec490-753e-46f9-9534-e65abf34dd2c",
   "metadata": {},
   "source": [
    "#### sklearn.feature_selection.r_regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a56d4a-faa6-455f-837f-2fbc1c89cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sk=r_regression(X,y)  # scikit-learn function\n",
    "r_sk=r_sk.reshape((len(pr_anom.lat),len(pr_anom.lon))) # reshape to 2D numpy array\n",
    "\n",
    "# notice r_sk is not in an xarray data structure here\n",
    "# so we can't use xarray plotting unless we convert r_sk to xarray\n",
    "# we could easily convert to xarray object using the same method as for y_xr above\n",
    "# I'm not doing that here so you can see a different way to plot\n",
    "\n",
    "# you will notice xarray does some things automatically to make plots look nicer that don't occur by default below, for example\n",
    "# - when using a 2-color cmap, xarray will automatically make the colorbar min and max extents the same so that the pos/neg color transition sits right at zero\n",
    "# - xarray will also label certain things based on the variable's metadata/labels. this doesn't really apply to the above, but thought I'd mention it\n",
    "\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())  # projection comes from cartopy package\n",
    "ax.coastlines(linewidth=0.5)  # coastlines come from cartopy package          \n",
    "plt.pcolormesh(pr_anom.lon.data,pr_anom.lat.data,r_sk,cmap='RdBu')  # this is a matplotlib.pyplot function, whereas before we were using .plot from xarray\n",
    "plt.title('correlation between monthly nino3.4 and precip anomalies 1950-2023')\n",
    "plt.colorbar(label='pearson r') # colorbar properties are passed this way\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df73c9-b082-427c-a4f4-eb219687af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set the colorbar limits so that zero falls at the color transition\n",
    "\n",
    "# plot\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree())\n",
    "ax.coastlines(linewidth=0.5)        \n",
    "plt.pcolormesh(pr_anom.lon.data,pr_anom.lat.data,r_sk,cmap='RdBu',vmax=r_sk.max(),vmin=-r_sk.max())\n",
    "plt.title('correlation between monthly nino3.4 and precip anomalies 1950-2023')\n",
    "plt.colorbar(label='pearson r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631b4f2-0902-4895-af99-fa16952c6381",
   "metadata": {},
   "source": [
    "#### custom function example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8dd63-8805-45ab-9a49-ee62571ae46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_r(timeseries,data_arr):\n",
    "    ''' accepts a 1D xarray timeseries with dim label 'time'\n",
    "        and a 2D or 3D xarray array with the same 'time' dim'''\n",
    "    \n",
    "    #1. Compute data length, mean and standard deviation along time axis: \n",
    "    n=timeseries.shape[0]\n",
    "    xmean = timeseries.mean('time')\n",
    "    ymean = data_arr.mean('time')\n",
    "    xstd  = timeseries.std('time')\n",
    "    ystd  = data_arr.std('time')\n",
    "                  \n",
    "    #2. Compute covariance along time axis\n",
    "    cov   =  np.sum((timeseries - xmean)*(data_arr - ymean), axis=0)/n\n",
    "    cov=cov.where(np.isfinite(ymean),np.nan) # np.sum sets the sum of all nan to 0, change these back to nan\n",
    "\n",
    "    #3. Compute correlation along time axis\n",
    "    cor   = cov/(xstd*ystd)\n",
    "    \n",
    "    # you could also just replace all of the above with one of the package functions we just covered\n",
    "    \n",
    "    #4. Compute p value\n",
    "    dof=n-2  # if you want to account for autocorrelation here's where you'd do it\n",
    "    tstats = cor*np.sqrt(dof)/np.sqrt(1-cor**2)  # students t\n",
    "    pval   = scipy.stats.t.sf(abs(tstats), dof)*2 # two tailed using effective sample size for dof\n",
    "    pval   = xr.DataArray(pval, dims=cor.dims, coords=cor.coords)    \n",
    "    \n",
    "    class output:\n",
    "        r=cor\n",
    "        p=pval\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fac3d-2108-4b54-b472-2f4097afb9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pearson_r(y_xr,pr_anom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a612cdc9-ca19-4481-9476-0763d90da19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation\n",
    "fig=plt.figure(figsize=(10,4))\n",
    "\n",
    "ax=fig.add_subplot(111,projection=ccrs.PlateCarree()) # projection comes from cartopy package\n",
    "ax.coastlines(linewidth=0.5)  # coastlines come from cartopy package      \n",
    "\n",
    "cbar_kwargs = {'label':'pearson r'} # modifications to the colorbar\n",
    "\n",
    "results.r.where(results.p<=siglvl).plot(ax=ax,cmap='RdBu',cbar_kwargs=cbar_kwargs) # this is xarray .where and .plot and colorbar properties have to be passed here with kwargs\n",
    "plt.title('Correlation between monthly precip and nino3.4 anomalies, pval<='+str(siglvl))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3860e-70ac-47dd-9c86-5a3701c9d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test to see if r and p from our custom function and different packages are identical\n",
    "# if identical, nothing prints\n",
    "\n",
    "npt.assert_allclose(results.r,r,rtol=0,atol=1e-5,err_msg='r scipy NOT EQUAL!')\n",
    "npt.assert_allclose(results.r,r_xr,rtol=0,atol=1e-5,err_msg='r_xr NOT EQUAL!')\n",
    "npt.assert_allclose(results.r,r_sk,rtol=0,atol=1e-5,err_msg='r_sk NOT EQUAL!')\n",
    "\n",
    "npt.assert_allclose(results.p,p,rtol=0,atol=1e-5,err_msg='p scipy NOT EQUAL!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18bd3f2-881b-43d1-9712-51f9a93d431e",
   "metadata": {},
   "source": [
    "# Rolling in time example\n",
    "\n",
    "### finding el nino / la nina events from nino3.4 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0b1af-ac88-446c-bcef-186afd0fe0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first do 5 month rolling mean\n",
    "nino_ind_xr=xr.DataArray(nino_ind,dims='time',coords={'time':time}) # numpy to xarray, using 1949 so we don't lose data with the rolling mean\n",
    "nino_rollmean=nino_ind_xr.rolling(time=5,center=False).mean()\n",
    "nino_rollmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f07c70-cc12-4e2a-a0c3-c6ac6c0af9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# el nino events are where nino_rollmean > 0.4 for 6 consecutive months \n",
    "# la nina events are where nino_rollmean <-0.4 for 6 consecutive months\n",
    "\n",
    "# first put the data in windows of length 6 months\n",
    "nino_windows=nino_rollmean.rolling(time=6,center=False).construct('window')\n",
    "nino_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1fb94-f467-4f2c-8e36-c758c78890ca",
   "metadata": {},
   "source": [
    "# other time window and fancy indexing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de4e11-741c-48de-8de5-ee707896d8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create boolean arrays where the index window meets the threshold for nino/nina events\n",
    "# the window is 6 months long, if all months above threshold, this collapses the window dimension and fills True in for the last month\n",
    "nino=(nino_windows>0.4).all(dim='window')\n",
    "nina=(nino_windows<-0.4).all(dim='window')\n",
    "nino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484aea7-4a72-4e36-9cf8-f9684f4810d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get all the nino/nina months we need to find each True and fill the 5 preceeding months with True also\n",
    "# this will give us the full length of each nino/nina event\n",
    "\n",
    "nino_window_true=np.argwhere(nino.data).squeeze()  # indexes of where nino is True\n",
    "nina_window_true=np.argwhere(nina.data).squeeze()  # indexes of where nino is True\n",
    "print(nino_window_true.shape, nino_window_true[0:5])\n",
    "\n",
    "# add indexes of the preceeding 5 months\n",
    "nino_list=list(nino_window_true) # numpy array to list\n",
    "nina_list=list(nina_window_true) # numpy array to list\n",
    "for i in range(1,6):\n",
    "    nino_list.extend(list(nino_window_true-i)) # add additional indexes to the list\n",
    "    nina_list.extend(list(nina_window_true-i)) # add additional indexes to the list\n",
    "    \n",
    "# since above could and does contain duplicates...\n",
    "nino_indexes=np.unique(nino_list) # this removes duplicates and sorts ascending\n",
    "nina_indexes=np.unique(nina_list) # this removes duplicates and sorts ascending\n",
    "nino_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2516092-89dc-4043-90a2-506e904ef0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix up nino and nina with this new info\n",
    "\n",
    "nino[nino_indexes]=True\n",
    "nina[nina_indexes]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36826968-5723-4683-97df-226c74e5ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask where nino event months = 1, nina event months = -1, and everything else = 0\n",
    "events=nino_rollmean.copy()\n",
    "events[:]=0  # set all values to 0\n",
    "\n",
    "events=xr.where(nino,1,events)  # fill with 1 where nino boolean array is True\n",
    "events=xr.where(nina,-1,events) # fill with -1 where nina boolean array is True\n",
    "# events=events.sel(time=slice(year_start,year_end)) # subset in time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b201c51-60c2-4c6e-841f-a0ec299f2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many of each value is in the events array\n",
    "unique, counts = np.unique(events, return_counts=True)\n",
    "for i in range(len(unique)):\n",
    "    print(unique[i],counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241dad7-85bd-4494-858e-013d767cda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the start and stop of each event\n",
    "\n",
    "nino_bounds=[]\n",
    "istart=nino_indexes[0]\n",
    "\n",
    "for i in range(nino_indexes.shape[0]-1):    \n",
    "    if nino_indexes[i+1]==nino_indexes[i]+1:\n",
    "        iend=nino_indexes[i+1]\n",
    "    else:\n",
    "        nino_bounds.append((istart,iend))\n",
    "        istart=nino_indexes[i+1]\n",
    "        \n",
    "        \n",
    "nina_bounds=[]\n",
    "istart=nina_indexes[0]\n",
    "\n",
    "for i in range(nina_indexes.shape[0]-1):    \n",
    "    if nina_indexes[i+1]==nina_indexes[i]+1:\n",
    "        iend=nina_indexes[i+1]\n",
    "    else:\n",
    "        nina_bounds.append((istart,iend))\n",
    "        istart=nina_indexes[i+1]        \n",
    "    \n",
    "nino_bounds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a7848a-0444-443a-a0d8-250984e5512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put tons of stuff on a plot\n",
    "\n",
    "fig=plt.figure(figsize=(20,4))\n",
    "plt.plot(time,nino_ind_xr, marker=10, markersize=3,linestyle='None') # monthly nino index as triangles\n",
    "plt.plot(time,nino_rollmean,linewidth=1)                             # 5-month rolling mean nino index as a line\n",
    "plt.axhline(y=0., color='grey', linestyle='--', linewidth=0.5)       # dashed grey line at zero\n",
    "plt.axhline(y=-0.4, color='grey', linestyle='-', linewidth=0.5)      # solid grey line at nina threshold \n",
    "plt.axhline(y=0.4, color='grey', linestyle='-', linewidth=0.5)       # solid grey line at nino threshold\n",
    "\n",
    "# blue shading during nino events\n",
    "for ievent in range(len(nino_bounds)):\n",
    "    time1=time[nino_bounds[ievent][0]]\n",
    "    time2=time[nino_bounds[ievent][1]]\n",
    "    plt.axvspan(time1,time2, color='cyan', alpha=0.25, lw=0)\n",
    "\n",
    "# yellow shading during nina events    \n",
    "for ievent in range(len(nina_bounds)):\n",
    "    time1=time[nina_bounds[ievent][0]]\n",
    "    time2=time[nina_bounds[ievent][1]]\n",
    "    plt.axvspan(time1,time2, color='gold', alpha=0.25, lw=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c88a90-83a5-448d-99a4-7eece60f9640",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Stacking usually means something slightly different in python than it does in the GIS world. Instead of meaning aligning data layers for computation, in python stacking usually means reducing dimensions. We saw this above in the correlation section when we changed our 3D array (time,lat,lon) to a 2D array of (time,space) with X=pr_anom.stack(space=('lat','lon')). In python, we put our data into multi-dimensional arrays where time is one dimension and lat, lon or space are dimensions as well. So, the \"stack\" terminology in the GIS/image processing world is really just the time dimension of our arrays. To calculate on multiple different 2- or 3-D arrays simultaneously we first usually regrid/resample to a common grid. Once our data is all in arrays on a common grid we can easily calculate things across the arrays. \n",
    "\n",
    "here our precip, tmin, tmax are all already on the same grid so we can skip resampling, everything has dimensions (time: 888, lat: 94, lon: 192)\n",
    "\n",
    "# Masking\n",
    "\n",
    "Masking is generally done with .where in python. \n",
    "\n",
    "There should be a .where available for whichever data structure that your data is in (e.g. xarray data array use xr.where, numpy array use np.where, pandas dataframe use pd.where, etc). \n",
    "\n",
    "[Numpy also has a module for constructing masked arrays](https://numpy.org/doc/stable/reference/maskedarray.generic.html). Personally, I never use this but am linking it here in case you want to look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a64e7-eb33-484a-bf09-67ca9caae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get more NCEP NCAR Reanalysis data\n",
    "ds1=xr.open_dataset(basedir+'temporary/NCEP-NCAR-1/tmin.2m.mon.mean.nc').sel(time=slice(year_start,year_end))\n",
    "tmin=ds1.tmin\n",
    "\n",
    "ds2=xr.open_dataset(basedir+'temporary/NCEP-NCAR-1/tmax.2m.mon.mean.nc').sel(time=slice(year_start,year_end))\n",
    "tmax=ds2.tmax\n",
    "\n",
    "tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5260e89-5dc4-4413-9b5b-db742530c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking example\n",
    "# at every month, find locations where \n",
    "# mean monthly precip is less than 10 mm/day and \n",
    "# mean monthly temperature range is greater than 20C\n",
    "\n",
    "# masking with xarray, returns an xarray object, retains metadata labels\n",
    "# each condition needs to be inside () like (condition)&(condition)&(condition)\n",
    "mask_xr=xr.where((pr<10)&((tmax-tmin)>20),1,0)\n",
    "\n",
    "# masking with numpy, returns a numpy object, losing all metadata labels\n",
    "mask_np=np.where((pr<10)&((tmax-tmin)>20),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b2cf9-ae6c-4960-8082-472ea241318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot xarray result\n",
    "fig=plt.figure(figsize=(20,4))\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree()) # 121 is rows,columns,plot_number\n",
    "ax.coastlines(linewidth=0.5)        \n",
    "mask_xr.sel(time='2020-08').plot(ax=ax) # this is xarray .plot \n",
    "plt.title('Aug 2020, mask_xr, where pr<10 & (tmax-tmin)>20')\n",
    "            \n",
    "\n",
    "# plot numpy result\n",
    "\n",
    "# what index in time is Aug 2020?\n",
    "t_ind=list(mask_xr.time.data).index(mask_xr.time.sel(time='2020-08').data)\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree()) # 122 is rows,columns,plot_number\n",
    "ax.coastlines(linewidth=0.5)      \n",
    "plt.pcolormesh(mask_xr.lon.data,mask_xr.lat.data,mask_np[t_ind,:,:])  # pcolormesh is from matplotlib.pyplot\n",
    "plt.colorbar()\n",
    "plt.title('Aug 2020, mask_np, where pr<10 & (tmax-tmin)>20')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3aa6e-e166-4011-9662-eced5c41b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the mask to a calculation (also done with .where)\n",
    "\n",
    "# calculate something only at the locations where mask=1\n",
    "# this could be any calculation, here we'll use the temperature range\n",
    "masked_calc=(tmax-tmin).where(mask_xr)  # xarray objects, xarray .where, result returned as xarray object with labels\n",
    "\n",
    "# plot result\n",
    "fig=plt.figure(figsize=(20,4))\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.coastlines(linewidth=0.5)        \n",
    "cbar_kwargs = {'label':'degrees C'} # modifications to the colorbar\n",
    "masked_calc.sel(time='2020-08').plot(ax=ax,cbar_kwargs=cbar_kwargs)  \n",
    "plt.title('Aug 2020, masked mean temperature range with xarray')\n",
    "\n",
    "\n",
    "# numpy version of above\n",
    "masked_calc_np=np.where(mask_np,(tmax.data-tmin.data),np.nan)  # numpy objects, numpy .where, result returned as numpy object (no labels)\n",
    "\n",
    "# plot result\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.coastlines(linewidth=0.5)      \n",
    "plt.pcolormesh(mask_xr.lon.data,mask_xr.lat.data,masked_calc_np[t_ind,:,:]) \n",
    "plt.colorbar(label='degrees C')\n",
    "plt.title('Aug 2020, masked mean temperature range with numpy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf566d-502d-4fa7-96b5-fdfca96cb943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduce the above but with discreet colorbars\n",
    "# this is easy to do with xarray plotting\n",
    "\n",
    "# type the list of levels out manually\n",
    "fig=plt.figure(figsize=(20,4))\n",
    "ax=fig.add_subplot(121,projection=ccrs.PlateCarree())\n",
    "ax.coastlines(linewidth=0.5)        \n",
    "cbar_kwargs = {'label':'degrees C'} # modifications to the colorbar\n",
    "masked_calc.sel(time='2020-08').plot(ax=ax,levels=[20,22,24,26,28],cbar_kwargs=cbar_kwargs)  \n",
    "plt.title('Aug 2020, masked mean temperature range')\n",
    "\n",
    "\n",
    "# use range to create a list of levels\n",
    "levels=range(21,29)\n",
    "\n",
    "ax=fig.add_subplot(122,projection=ccrs.PlateCarree())\n",
    "ax.coastlines(linewidth=0.5)        \n",
    "cbar_kwargs = {'label':'degrees C'} # modifications to the colorbar\n",
    "masked_calc.sel(time='2020-08').plot(ax=ax,levels=levels,cbar_kwargs=cbar_kwargs)  \n",
    "plt.title('Aug 2020, masked mean temperature range')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd2ce7-6cd4-49df-9f37-3de2ea96b310",
   "metadata": {},
   "source": [
    "# focal operations\n",
    "\n",
    "I'm not too familiar with this but I think in python, focal operations are generally offered in image processing packages.  Some packages that may be useful are:\n",
    "- [scipy.ndimage](https://docs.scipy.org/doc/scipy/reference/ndimage.html) works on numpy arrays, I would start here\n",
    "- [dask_image.ndfilters](https://image.dask.org/en/latest/dask_image.ndfilters.html) scipy.ndimage functions written for dask arrays (chunked big data)\n",
    "- [focal_stats](https://focal-stats.readthedocs.io/en/latest/api.html) maybe the simplest, but limited functions. basically a numpy extension\n",
    "- [xarray-spatial](https://pypi.org/project/xarray-spatial/) basically an xarray extension. This is new and it looks like access to the documentation may not be available yet. I've never used this but might be something to keep an eye on.\n",
    "- [PIL/pillow](https://pillow.readthedocs.io/en/stable/) basic processing for data in image formats\n",
    "- [scikit image](https://scikit-image.org/docs/stable/) image processing, lots of functions but maybe not great for working with a time dimension\n",
    "- [opencv](https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html) probably similar to scikit image? I've never used it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataprep",
   "language": "python",
   "name": "dataprep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
