{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeafde57-fa05-4102-9de4-a7a980d2d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.distributed import Client,LocalCluster\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e972ae-55eb-4809-8e93-4124234cfef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '/path/to/our/shared/datasets/dir/processed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10834136-a5e6-4e94-ba2a-c23e5cfea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers=40 \n",
    "cluster=LocalCluster(n_workers=nworkers,threads_per_worker=1) \n",
    "client=Client(cluster) \n",
    "client.wait_for_workers(n_workers=nworkers,timeout=10) \n",
    "client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f22b3-34d9-492d-86f8-d902431aaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_reg(data_chunk, min_obs = 10):\n",
    "\n",
    "    # create 3D x values with nans where y is nan and where total obs <10\n",
    "    # eliminate the intermediate step of non_valid will save some RAM\n",
    "    # using isfinite instead of ~np.isnan is more direct but would also account for inf values if there were any\n",
    "    n_obs = np.isfinite(data_chunk).sum('time') # 2D, range 0-19 no nans, retains xy coord labels\n",
    "    n_obs_clean = n_obs.where(n_obs > min_obs)  # put nan where too few total obs  \n",
    "    x = data_chunk.time.dt.year.expand_dims(dim = {\"y\": data_chunk.y, \"x\": data_chunk.x}, axis = (1, 2)) # to 3D (time,y,x)\n",
    "\n",
    "    # fill x and y with nan where data is missing and where less than 10 total obs\n",
    "    x_clean =x.where( (np.isfinite(data_chunk)) & (np.isfinite(n_obs_clean)) ) \n",
    "    y_clean = data_chunk.where(np.isfinite(n_obs_clean))\n",
    "    \n",
    "    # # linear regression\n",
    "    y_mean = y_clean.mean('time')\n",
    "    y_var = ((y_clean - y_mean)**2).sum(dim = 'time')/n_obs_clean\n",
    "    y_std = np.sqrt(y_var)\n",
    "    x_mean = x_clean.mean('time')\n",
    "    x_var = ((x_clean - x_mean)**2).sum(dim = 'time')/n_obs_clean\n",
    "    x_std = np.sqrt(x_var)\n",
    "\n",
    "    cov = ((x_clean - x_mean)*(y_clean - y_mean)).sum(dim = 'time')/n_obs_clean\n",
    "    cor = cov/(x_std*y_std)\n",
    "    slope = cov/(x_std**2)\n",
    "\n",
    "    # significance\n",
    "    t_stats = cor*np.sqrt(n_obs_clean - 2)/np.sqrt(1 - cor**2)\n",
    "    p = scipy.stats.t.sf(abs(t_stats), n_obs_clean - 2)*2\n",
    "    p = xr.DataArray(p, dims = cor.dims, coords = cor.coords)\n",
    "\n",
    "    # convert results to a dataset\n",
    "    result = slope.to_dataset(name = 'wealth_pc_trend').assign(p_value = p).assign(n_obs = n_obs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f748f-9632-4f95-8591-e10780eab6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('lazy load data')\n",
    "# lazy load wealth to chunked array (not in memory)\n",
    "wealth = xr.open_dataset(path_to_data+'wealth.nc',chunks={'time':-1,'y':1000,'x':1000}).wealth_pc\n",
    "\n",
    "# set up a chunked dataset template that has the exact dims, coords, and variable names as the output of custom_reg\n",
    "print('create output template')\n",
    "template=wealth.isel(time=0).drop_vars(['time'])\n",
    "template.attrs = {'standard_name': 'wealth_pc_trend',\n",
    "                  'long_name': 'Annual time trend for the per capita wealth'}\n",
    "template.name=template.attrs['standard_name']\n",
    "template=template.to_dataset()\n",
    "template['p_value']=template['wealth_pc_trend']\n",
    "template['n_obs']=template['wealth_pc_trend']\n",
    "\n",
    "# do the parallel compute\n",
    "print('execute')\n",
    "wealth_trend=wealth.map_blocks(custom_reg,template=template,kwargs={'min_obs':10}).compute()\n",
    "wealth_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48319eb-2bf7-442f-92c5-3c0a40cf288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a subset where significant\n",
    "wealth_trend.wealth_pc_trend.sel(y=slice(2E6,0.34E6),x=slice(0.34E6,2E6)).where(wealth_trend.p_value<0.1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0e0ea-1262-4979-ad5e-6921b37a1185",
   "metadata": {},
   "source": [
    "# compare to polyfit_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e797c7-16cc-4eca-ad53-98df21612258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call with map_blocks\n",
    "# operates on xarray chunks and returns xarray chunks\n",
    "def polyfit_parallel(data_chunk,skipna):\n",
    "    data_chunk.coords['datetime']=data_chunk.time\n",
    "    data_chunk.coords['time']=data_chunk.datetime.dt.year\n",
    "    result_chunk = data_chunk.polyfit('time',1,skipna=skipna)\n",
    "    return result_chunk.polyfit_coefficients.sel(degree=1).drop_vars('degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e699e20-925b-4064-b010-b6beb21a5025",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('lazy load data')\n",
    "# lazy load wealth to chunked array (not in memory)\n",
    "wealth = xr.open_dataset(path_to_data+'wealth.nc',chunks={'time':-1,'y':1000,'x':1000}).wealth_pc\n",
    "\n",
    "# set up a chunked array template that has the exact dims and coords as the output of function polyfit_parallel will have\n",
    "print('create output template array')\n",
    "template=wealth.isel(time=0).drop_vars(['time','spatial_ref'])\n",
    "template.attrs={'standard_name':'wealth_pc trend'} # put whatever attributes you want\n",
    "\n",
    "# do the parallel compute\n",
    "print('execute')\n",
    "wealth_trend_polyfit=wealth.map_blocks(polyfit_parallel,template=template,kwargs={'skipna':'True'}).compute()\n",
    "wealth_trend_polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66c4df-eeef-43fb-b51e-a06396dc9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,6))\n",
    "\n",
    "ax=fig.add_subplot(131)\n",
    "wealth_trend.wealth_pc_trend.sel(y=slice(2E6,0.34E6),x=slice(0.34E6,2E6)).plot()\n",
    "plt.title('custom_reg')\n",
    "\n",
    "ax=fig.add_subplot(132)\n",
    "wealth_trend_polyfit.sel(y=slice(2E6,0.34E6),x=slice(0.34E6,2E6)).plot()\n",
    "plt.title('polyfit_parallel')\n",
    "\n",
    "ax=fig.add_subplot(133)\n",
    "(wealth_trend.wealth_pc_trend - wealth_trend_polyfit).sel(y=slice(2E6,0.34E6),x=slice(0.34E6,2E6)).plot(cmap = 'bwr')\n",
    "plt.title('custom_reg minus polyfit_parallel')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4729b3-5760-423e-88a9-805b9b69badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=wealth_trend.wealth_pc_trend - wealth_trend_polyfit\n",
    "diff.min().item(), diff.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe523076-2840-4d55-a3ac-9a02a9f9a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.restart()\n",
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d283cc-0e9f-4fea-9829-91a2cfcc465b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sera_dataprep2",
   "language": "python",
   "name": "sera_dataprep2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
